name: Premier League Data Pipeline
# Updated Python version to 3.10 for Streamlit compatibility
on:
  schedule:
    - cron: '0 7 * * *'  # 2 AM ET (7 AM UTC) daily
  workflow_dispatch:  # Allow manual triggering

permissions:
  contents: write  # Allow the workflow to commit and push changes

jobs:
  check-matches:
    runs-on: ubuntu-latest
    outputs:
      has-matches: ${{ steps.check.outputs.has-matches }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-ci.txt

      - name: Fetch upcoming fixtures
        run: python fetch_upcoming_fixtures.py

      - name: Check for tomorrow's matches
        id: check
        run: |
          if python check_tomorrow_matches.py; then
            echo "has-matches=true" >> $GITHUB_OUTPUT
            echo "Matches found for tomorrow - proceeding with data update"
          else
            echo "has-matches=false" >> $GITHUB_OUTPUT
            echo "No matches tomorrow - skipping data update"
          fi

  update-data:
    needs: check-matches
    if: needs.check-matches.outputs.has-matches == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 60  # 1 hour total job timeout

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-ci.txt

      - name: Run data pipeline
        timeout-minutes: 15  # 15 minutes for data processing
        run: |
          echo "Running data pipeline..."
          python combine_raw_data.py
          python prepare_model_data.py

      - name: Train ML models
        timeout-minutes: 30  # 30 minutes for model training
        run: |
          echo "Training ML models..."
          python train_models.py

      - name: Validate models
        run: |
          echo "Validating trained models..."
          python -c "
          import os
          import pickle
          models_dir = 'models/'
          
          # Required models (must be present)
          required_models = ['xgb_baseline.pkl', 'ensemble_model.pkl', 'optimized_xgb.pkl', 'model_performance.pkl']
          
          # Optional models (neural network might fail)
          optional_models = ['neural_model.pkl', 'neural_scaler.pkl']
          
          missing_required = []
          missing_optional = []
          
          for model_file in required_models:
              if not os.path.exists(os.path.join(models_dir, model_file)):
                  missing_required.append(model_file)
                  
          for model_file in optional_models:
              if not os.path.exists(os.path.join(models_dir, model_file)):
                  missing_optional.append(model_file)
          
          if missing_required:
              raise FileNotFoundError(f'Missing required model files: {missing_required}')
              
          if missing_optional:
              print(f'⚠️  Optional model files not found (this is OK): {missing_optional}')
          else:
              print('✅ All model files present (including optional neural network)')
              
          # Load and check performance
          with open(os.path.join(models_dir, 'model_performance.pkl'), 'rb') as f:
              perf = pickle.load(f)
          print('Model performance loaded successfully')
          for model_name, metrics in perf.items():
              print(f'{model_name}: Acc={metrics[\"accuracy\"]:.3f}, MAE={metrics[\"mae\"]:.3f}')
          "

      - name: Validate data
        run: |
          echo "Validating updated data..."
          python -c "
          import pandas as pd
          import os
          csv_path = 'data_files/combined_historical_data_with_calculations.csv'
          if os.path.exists(csv_path):
              df = pd.read_csv(csv_path, sep='\t')
              print(f'Data validation passed: {len(df)} rows')
              if len(df) < 1000:
                  raise ValueError('Data validation failed: insufficient rows')
          else:
              raise FileNotFoundError('Data file not found after pipeline')
          "

      - name: Commit and push changes
        run: |
          echo "Checking for changes..."
          git config --local user.email 'action@github.com'
          git config --local user.name 'GitHub Action'

          # Check if there are changes
          if git diff --quiet && git diff --staged --quiet; then
            echo "No changes to commit"
          else
            echo "Committing data and model updates..."
            git add data_files/
            git add models/
            git commit -m "Nightly update - $(date +'%Y-%m-%d') - Data + Models" || echo "No changes to commit"
            git push
            echo "Data and model updates completed and pushed"
          fi

      - name: Notification on failure
        if: failure()
        run: |
          echo "Data pipeline failed - check logs for details"
          # Could add Slack/email notifications here in the future

  validate-predictions:
    needs: update-data
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-ci.txt

      - name: Validate predictions
        run: |
          echo "Validating prediction accuracy..."
          python -c "
          from track_predictions import validate_predictions
          import pandas as pd
          
          # Run validation
          perf = validate_predictions()
          
          if perf is not None and len(perf) > 0:
              completed = perf[perf['Correct'].notna()]
              if len(completed) > 0:
                  accuracy = completed['Correct'].mean()
                  print(f'✅ Prediction validation completed: {accuracy:.1%} accuracy ({len(completed)} predictions)')
              else:
                  print('ℹ️  No predictions ready for validation yet')
          else:
              print('ℹ️  No prediction history found')
          "

      - name: Commit prediction validation updates
        run: |
          echo "Checking for prediction log changes..."
          git config --local user.email 'action@github.com'
          git config --local user.name 'GitHub Action'

          # Check if predictions log has changes
          if git diff --quiet data_files/predictions_log.csv 2>/dev/null; then
            echo "No prediction validation updates to commit"
          else
            echo "Committing prediction validation updates..."
            git add data_files/predictions_log.csv
            git commit -m "Prediction validation update - $(date +'%Y-%m-%d')" || echo "No changes to commit"
            git push
            echo "Prediction validation updates committed and pushed"
          fi